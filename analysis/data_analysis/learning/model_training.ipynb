{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper that helps us run a few types of models on a few splits of\n",
    "data. The outputs of this script are (1) saved features and (2) a trained model,\n",
    "selected to have the best dev set score. The script is exactly the same as the model training\n",
    "for the simulation, except some of the paths and configuration files have been changed.\n",
    "\n",
    "The main input parameters are the path to the `train_yaml` (relative to the root directory) and the bootstrap index to use. Since we may want to run this notebook as a python script (using `nbconvert`) we look up these arguments using environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from addict import Dict\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../../../../inst/python\")\n",
    "from data import initialize_loader\n",
    "from models.vae import VAE, vae_loss\n",
    "from models.cnn import CBRNet, cnn_loss\n",
    "import models.random_features as rcf\n",
    "import train as st\n",
    "import train_rcf as srcf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim\n",
    "import yaml\n",
    "\n",
    "train_yaml = Path(\"conf/tnbc_rcf-k256.yaml\")\n",
    "bootstrap = 1\n",
    "data_dir = Path(\"../../../data/raw_data/stability_data/\")\n",
    "save_dir = Path(\"../../../data/derived_data/tnbc_models\") / train_yaml.name.replace(\".yaml\", \"\") / str(bootstrap)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "(save_dir / \"features\" / \"logs\").mkdir(parents=True, exist_ok=True)\n",
    "opts = Dict(yaml.safe_load(open(train_yaml, \"r\")))\n",
    "print(opts.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume that the data have already been preprocessed using the `prepare_mibi.Rmd` document, also in this `data_analysis` folder. We have provided a saved version of these output in the `stability_data_tnbc.tar.gz` archive. The block below is unzipping these data so that they can be referred to during model training. Note that this will overwrite any previously unzipped simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd ../../data/raw_data/\n",
    "!rm -rf stability_data/\n",
    "!tar -zxvf stability_data_tnbc.tar.gz\n",
    "%cd ../../data_analysis/learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create directories for saving all the features. We'll also read in all paths for training / development / testing. This is a bit more involved than the usual training process, since we'll want loaders specifically for looking at changes in feature activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = data_dir / opts.organization.features_dir\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "splits = pd.read_csv(data_dir / opts.organization.splits)\n",
    "resample_ix = pd.read_csv(data_dir / opts.bootstrap.path)\n",
    "\n",
    "paths = {\n",
    "    \"train\": splits.loc[splits.split == \"train\", \"path\"].values[resample_ix.loc[bootstrap]],\n",
    "    \"dev\": splits.loc[splits.split == \"dev\", \"path\"].values,\n",
    "    \"test\": splits.loc[splits.split == \"test\", \"path\"].values,\n",
    "    \"all\": splits[\"path\"].values\n",
    "}\n",
    "\n",
    "np.random.seed(0)\n",
    "save_ix = np.random.choice(len(splits), opts.train.save_subset, replace=False)\n",
    "loaders = {\n",
    "    \"train_fixed\": initialize_loader(paths[\"train\"], data_dir, opts),\n",
    "    \"train\": initialize_loader(paths[\"train\"], data_dir, opts, shuffle=True),\n",
    "    \"dev\": initialize_loader(paths[\"dev\"], data_dir, opts),\n",
    "    \"test\": initialize_loader(paths[\"test\"], data_dir, opts),\n",
    "    \"features\": initialize_loader(paths[\"all\"][save_ix], data_dir, opts)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model and the loss functions. This is not super elegant, basically a long switch statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.train.model == \"cnn\":\n",
    "    model = CBRNet(nf=opts.train.nf, p_in=opts.train.p_in)\n",
    "    loss_fn = cnn_loss\n",
    "elif opts.train.model == \"vae\":\n",
    "    model = VAE(z_dim=opts.train.z_dim, p_in=opts.train.p_in)\n",
    "    loss_fn = vae_loss\n",
    "elif opts.train.model == \"rcf\":\n",
    "    patches = rcf.random_patches([data_dir / p for p in paths[\"train\"]], k=opts.train.n_patches)\n",
    "    model = rcf.WideNet(patches)\n",
    "else:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's prepare a logger to save the training progress. We also save the indices of the samples for which we'll write activations -- it would be too much (and not really necessary) to write activations for all the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_path = data_dir / opts.organization.features_dir / \"subset.csv\"\n",
    "splits.iloc[save_ix, :].to_csv(subset_path)\n",
    "writer = SummaryWriter(features_dir / \"logs\")\n",
    "writer.add_text(\"conf\", json.dumps(opts))\n",
    "out_paths = [\n",
    "    save_dir / opts.organization.features_dir, # where features are saved\n",
    "    save_dir / opts.organization.metadata, # metadata for features (e.g., layer name)\n",
    "    save_dir / opts.organization.model # where model gets saved\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model. Training for the random convolutional features model is just ridge regression -- there are no iterations necessary. For the CNN and VAE, all the real logic is hidden away in the `st.train` function. The trained model and extracted features get saved into the `save_dir` folder. To save features across many runs, we rerun this notebook across many values of the `bootstrap` parameter. We find this step worth parallelizing on a computer cluster. The HTCondor submit scripts used in our paper are available [here](https://github.com/krisrs1128/learned_inference/blob/master/run_scripts/train.submit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.train.model == \"rcf\":\n",
    "    srcf.train_rcf(model, loaders, out_paths, alpha=opts.train.alpha, l1_ratio=opts.train.l1_ratio, normalize=True)\n",
    "else:\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=opts.train.lr)\n",
    "    st.train(model, optim, loaders, opts, out_paths, writer, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "li",
   "language": "python",
   "name": "li"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
